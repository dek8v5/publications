Agricultural 3D reconstruction is often achieved by using
Structure from Motion of high resolution RGB imagery from UAV or hand
held cameras \citep{guo2021,che2020}.  Light Detector and Ranging
(LiDAR) is a depth sensor-based device that uses a pulsed laser to
measure distances to the surfaces of objects. It is less sensitive to
lighting conditions than RGB imagery, making it a good candidate for
agricultural and outdoor applications.  Morphological phenotypes, such
as plant height, leaf pathologies and dimensions, and the status of
reproductive organs are important to assess as the plant develops for
research in genetics and crop improvement and for managing production
fields \citep{kelly2016a}. In principle, LiDAR data could provide
detailed morphological information for a variety of crops, and the
dimensions obtained from LiDAR-based morphological models could be used
to correct morphological models built from other data. However,
reflections from beneath the canopy, for example for stems and lower
leaves, are currently difficult to resolve underneath the complex
occlusions.  The recent availability of consumer-grade LiDAR devices on
Apple iPhones and iPads might provide a way to collect higher resolution
data close to the plant, rather than imaging through the canopy.  Gollob
/et alia/ compared data on the diameter of trees at breast height (dbh,
a standard forestry measure) collected with a 2020 iPad Pro to standard
methods and found remarkably good agreement \citep{gollob2021}.



We explored the use of a 2021 Apple iPad Pro, different methods of data
collection, and different free apps on maize, soybeans, and common field
weeds late in the 2021 field season.  These plants have very different
architectures: maize is tall, with relatively few, large, fairly thick,
and quite narrow leaves; soybeans are short and bushy, with many small,
thin, roughly circular leaves.  Cocklebur, a common field weed, is
intermediate: taller than most soybean varieties, with much larger
circular and thicker leaves, and a more open, branching stem.  The two
best apps we found were Scaniverse and \tdsapp.  For \tdsapp, the best
settings were high resolution (5mm), maximum depth 5m, confidence level
high or medium (depending on the plants), and masking set to off.  For
Scaniverse, the only settable parameter was the maximum depth, which we
set to 5m, and we have no information on the app's default point cloud
resolution.  We scanned isolated maize plants from the ground to the
tassel, walking around the plant to capture data at different heights
and distances from the culm. For soybeans and cocklebur, we scanned from
the top of the canopy downwards, circling around the plant.  We imaged
these at distances ranging from 5cm -- 1m, while we imaged maize at
about 3m from the culm.  All scans used an oblique angle and included
the soil for ample ground reflections.


Scans of cocklebur and soybeans show their canopies reflect the LiDAR
well, but the lower leaves and the stems were invisible to LiDAR.  When
set to high confidence, \tdsapp returned fewer features than at medium
confidence; conversely, its 3D reconstruction was better at the higher
confidence.  Scaniverse's default higher confidence level detects fewer
features, producing a poorer 3D reconstruction than \tdsapp.  In
contrast, maize was scanned better with Scaniverse than with \tdsapp.
Plant features more proximal to the culm, such as the ear and the leaf
sheath, were reconstructed better than more distal portions of the
leaves.  Senesced maize --- culms, ears, and leaves --- reflected better
compared to younger, greener leaves and tassels.  These results are
consistent with prior observations: thicker, denser plant structures
reflect better than thinner, less dense ones.  As maize senesces, it
dries and shrinks, likely increasing the density of the plant material.
For the bushier plants, we could partially compensate for the canopy
occluding the laser by scanning alongside the plant at close range, but
this strategy failed with maize.  Reconstruction may be better with
better instrumentation, but it seems likely that better models of plant
reflectances, gap probabilities, and leaves will be needed for each crop
species of interest.


