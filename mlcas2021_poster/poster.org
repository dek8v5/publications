# this is ../b/artistry/papers/current/mlcas2021_poster/poster.org


# make sure you export correctly:  C-c C-e l P
#
# then process with pdflatex as normally



* 90" talk outline

# #+title: 3D plant morphology in the field: experiments with a consumer LiDAR device
# #+author: Dewi Endah Kharismawati, Chimdi Walter Ndubuisi, and Toni Kazic
# #+date: \today


# ################### preamble for beamer slides #######################

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation, aspectratio=54]
#+BEAMER_HEADER: \title[3D Field Plants with LiDAR]{3D plant morphology in the field: experiments with a consumer LiDAR device}
#+BEAMER_HEADER: \author[Kharismawati \emph{et al.}]{Dewi Endah Kharismawati, Chimdi Walter Ndubuisi, and Toni Kazic}
#+BEAMER_HEADER: \institute{University of Missouri}

#+LATEX_HEADER: \usepackage{graphbox,graphics,graphicx,rotating,xcolor,pgf,tikz,pgfplots}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \def\tdsapp{3D Scanner App\xspace}


#+BEAMER_HEADER_EXTRA: \title[3D Plants with LiDAR]{3D plant morphology in the field: experiments with a consumer LiDAR device}
#+BEAMER_HEADER_EXTRA: \author[Kharismawati \emph{et al.}]{Dewi Endah Kharismawati, Chimdi Walter Ndubuisi, and Toni Kazic}

#
#
#
# pick a beamer theme you like!  There are ways of truncating the material
# in the footer, and omitting the navigation.  Decide first what you like
# and then we can adjust. See
#
# https://www.overleaf.com/gallery/tagged/beamer for some nice ideas and
# also
#
# https://hartwork.org/beamer-theme-matrix/
#
# and https://github.com/martinbjeldbak/ultimate-beamer-theme-list
#
# and also google "modern beamer themes"
#
# last talk I gave is here:
# ../../../talks/toni/20/giovanna_intl_xchng/slides.tex
# so you can see how that turned out (just one slide)
#
#+latex_header: \mode<beamer>{\usetheme{Madrid}}

#+LaTeX_CLASS_OPTIONS: [bigger]





#+OPTIONS: H:2 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t
#+OPTIONS: TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc



#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:https://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+BEAMER_FRAME_LEVEL: 2





#+LATEX_HEADER: \def\lb#1{../../../../../../../home/dek8v5/Dropbox/graphos_reprints/references/bibliography/#1.bib}   
#+LATEX_HEADER: \def\tlb#1{../../../../../../Dropbox/graphos_reprints/references/bibliography/#1.bib}   
#+LATEX_HEADER: \def\gb#1{../../../../zu_lesen/references/bibliography/#1.bib}

#+LATEX_HEADER: \def\bp{\lb}









** Rationale

   + 3D reconstructions often use Structure from Motions (SfM) of 360$^o$
     RGB videos, but can benefit from depth information.
   + LiDAR is less sensitive to ambient light, making it a
     very good candidate for agricultural applications.
   + LiDAR is now available in consumer grade devices, such as
     Apple's iPads and iPhones.
   + There are many free applications to get 3D point clouds.
   + We explored 2 apps for collecting point clouds of field plants:
     \tdsapp and Scaniverse.


** Point Cloud Quality depends on Plant Morphology

#+begin_export latex
\centering

% this is ../b/artistry/papers/current/mlcas2021_poster/images/lidar_tableau.tex
%
% this is the transposed figure we used in the extended abstract
%
% Kazic, 27.10.2021



% fixed vertical alignment with graphbox per
% https://tex.stackexchange.com/questions/19080/how-to-vertically-center-text-with-an-image-in-the-same-row-of-a-table
%
% Kazic, 2.10.2021



%
\scalebox{0.75}{%
\begin{tabular}{cccc}
%
% this is the transpose: plants as rows and outputs as columns
%
% rgb
%
& & \multicolumn{2}{c}{raw point clouds} \\
& \textsc{RGB} & \tdsapp & Scaniverse \\
%
\emph{soybean} &
\scalebox{0.08}{\includegraphics[align=c]{./images/meshlab/soybean3_rgb.png}} &
\scalebox{0.10}{\includegraphics[align=c]{./images/meshlab/soybean3_3dscanner_pc.png}} &
\scalebox{0.10}{\includegraphics[align=c]{./images/meshlab/soybean3_scaniverse_pc.png}}  \\
%
\emph{cocklebur} &
\scalebox{0.175}{\includegraphics[align=c]{./images/meshlab/cocklebur2_rgb.png}} &
\scalebox{0.11}{\includegraphics[align=c]{./images/meshlab/cocklebur2_3dscanner_pc.png}} &
\scalebox{0.115}{\includegraphics[align=c]{./images/meshlab/cocklebur2_scaniverse_pc.png}} \\
%
\emph{maize} &
\scalebox{0.14}{\includegraphics[align=c]{./images/meshlab/corn2_rgb.png}} &
\scalebox{0.125}{\includegraphics[align=c]{./images/meshlab/corn2_3dScanner_pc.png}} &
\scalebox{0.16}{\includegraphics[align=c]{./images/meshlab/corn2_scaniverse_pc.png}}\\
%
\end{tabular}
}





#+end_export


** More Points make Smoother Plants

# correct?????

#+attr_latex: :align l|rr
| plant     | \tdsapp | Scaniverse |
|-----------+---------+------------|
| soybean   | 197,819 | 1,289,299  |
| cocklebur | 53,913  | 1,102,817  |
| maize     | 96,807  | 1,503,680  |


** Acknowledgments

   + Missouri Maize Center and especially Chris Browne
   + Dept. of Electrical Engineering and Computer Science Graduate
     Fellowship
   + an anonymous gift


* beamer mechanics :noexport:

[[https://orgmode.org/worg/exporters/beamer/tutorial.html][Fraga's tutorial, pretty old]]


[[https://github.com/fniessen/refcard-org-beamer][Niessen's cheat sheet]]

[[https://tippenhauer.de/post/writing-presentations-in-org-mode-markup/][more recent tutorial]]

[[https://geeksocket.in/posts/presentations-org-emacs/#using-org-tree-slide][interesting]];
[[https://sachachua.com/blog/2013/04/how-to-present-using-org-mode-in-emacs/][
presenting
straight in emacs --- it may be possible to call meshlab from an emacs code
block, I don't know]]


https://orgmode.org/manual/Beamer-specific-syntax.html
https://orgmode.org/manual/Frames-and-Blocks-in-Beamer.html
https://github.com/fniessen/refcard-org-beamer


https://karl-voit.at/2019/10/26/all-things-org/
https://github.com/jkitchin/org-ref



* abstract including mlcas preamble                                :noexport:

# ############ preamble for printed abstract per MLCAS #########################

# #+LATEX_CLASS_OPTIONS: [11pt]
# #+LATEX_HEADER: \usepackage[text={6.5in,9in},centering]{geometry}
# #+LATEX_HEADER: \usepackage{microtype}
# #+LATEX_HEADER: \usepackage[T1]{fontenc}
# #+LATEX_HEADER: \usepackage{newtxtext,newtxmath}
# #
# # #+LATEX_HEADER: \usepackage[reqno]{amsmath}
# # #+LATEX_HEADER: \usepackage{amsfonts,amssymb,amscd,mathtools,mathrsfs,bm}
# #
# #+LATEX_HEADER: \usepackage{cite,authblk}
# #+LATEX_HEADER: \usepackage{graphbox,graphics,graphicx,rotating,xcolor,pgf,tikz,pgfplots}
# #+LATEX_HEADER: \usepackage{IEEEtrantools,multirow,float,booktabs,tabulary}
# #+LATEX_HEADER: \usepackage{}
# #+LATEX_HEADER: \usepackage{xspace}
# #+LATEX_HEADER: \usepackage[nottoc]{tocbibind}
# #+LATEX_HEADER: \usepackage{species}




# # their header

# #+LATEX_HEADER: \usepackage{fancyhdr}
# #+LATEX_HEADER: \pagestyle{fancy}
# #+LATEX_HEADER: \fancyhead[LE,LO]{\textsc{EXTENDED ABSTRACT - MLCAS 2021 – ONLINE – 02 \~{} 04.11.2021}}
# #+LATEX_HEADER: \fancyhead[RE,RO]{}
# #+LATEX_HEADER: \fancyfoot{}
# #+LATEX_HEADER: \renewcommand{\headrulewidth}{0pt}

# #+LATEX_HEADER: \def\mng#1{\Large{\textcolor{red}{\textbf{#1}}}}
# #+LATEX_HEADER: \def\tdsapp{3D Scanner App\xspace}


# #+LATEX_HEADER: \usepackage{natbib}



# #+OPTIONS: toc:nil
# #+OPTIONS: num:nil
# #+OPTIONS: title:nil author:nil date:nil

# #+LATEX_HEADER: \usepackage[colorlinks=true,urlcolor=violet,linkcolor=red,citecolor=blue]{hyperref}



# haven't found how to customize the title/author/etc for latex export from
# org
#
# this doesn't work:
# from https://emacs.stackexchange.com/questions/47347/customizing-org-latex-title-command-to-edit-title-page
#
# nor jiggling hyperref
#
# So just bang it in directly in latex
#
# Kazic, 1.10.2021




# we are paper id 10



# specify exact font size, rather than the descriptive sizes, per
# https://www.sascha-frank.com/latex-font-size.html
#
# Kazic, 1.10.2021



#+begin_export latex

% \title{\fontsize{14}{16} \selectfont \textbf{3D plant morphology in the field: experiments with a consumer LiDAR device}}
\title{\huge{\textbf{3D plant morphology in the field: experiments with a consumer LiDAR device}}}
\author{\fontsize{12}{14} \selectfont \textbf{Dewi Endah Kharismawati$^{\ast}$}}
\author{\fontsize{12}{14} \selectfont \textbf{Chimdi Walter Ndubuisi}}
\author{\fontsize{12}{14} \selectfont \textbf{Toni Kazic}}
\affil{\emph{Dept.\, of Electrical Engineering and Computer Science, University of Missouri, Columbia, USA} \\
\emph{Missouri Maize Center} \\
\emph{Interdisciplinary Plant Group} \\ 
\emph{Plant Science Foundry}}
\maketitle
\thispagestyle{fancy}

\vspace{-1cm}

#+end_export


# 3--5 keywords

*Keywords:* 3D reconstruction, plant morphology, plant phenotyping, LiDAR,
field crops


*Abstract:* Agricultural 3D reconstruction is often achieved by using Structure from
Motion of high resolution RGB imagery from UAV or hand held cameras
\citep{guo2021,che2020}.  Light Detector and Ranging (LiDAR) is a depth
sensor-based device that uses a pulsed laser to measure distances to the
surfaces of objects. It is less sensitive to lighting conditions than RGB
imagery, making it a good candidate for agricultural and outdoor
applications.  
#
# LiDAR sensors are often mounted on aerial vehicles to get
# point clouds that reconstruct 3D canopies of the area of interest
# (\cite{schneider2014,yin2020}).  Successful reconstruction depends on having
# an adequate model for the reflectances and scattering of the vegetation,
# the relevant scanner and image acquisition parameters, and good simulations
# of expected waveforms, point clouds, and intensities.
#
Morphological phenotypes, such as plant height, leaf pathologies and
dimensions, and the status of reproductive organs are important to
assess as the plant develops for research in genetics and crop
improvement and for managing production fields \citep{kelly2016a}. In
principle, LiDAR data could provide detailed morphological information
for a variety of crops, and the dimensions obtained from LiDAR-based
morphological models could be used to correct morphological models
built from other data. However, reflections from beneath the canopy,
for example for stems and lower leaves, are currently difficult to
resolve underneath the complex occlusions.
#
The recent availability of consumer-grade LiDAR devices on
Apple iPhones and iPads might provide a way to collect
higher resolution data close to the plant, rather than imaging through
the canopy.  Gollob /et alia/ compared data on the diameter of trees
at breast height (dbh, a standard forestry measure) collected with a
2020 iPad Pro to standard methods and found remarkably good agreement
\citep{gollob2021}.  



# Relatively high resolution LiDAR scanners are scarce and
# expensive. 


We explored the use of a 2021 Apple iPad Pro, different methods of
data collection, and different free apps on maize, soybeans, and
common field weeds late in the 2021 field season.  These plants have
very different architectures: maize is tall, with relatively few,
large, fairly thick, and quite narrow leaves; soybeans are short and
bushy, with many small, thin, roughly circular leaves.  Cocklebur, a
common field weed, is intermediate: taller than most soybean
varieties, with much larger circular and thicker leaves, and a more
open, branching stem.
#
The two best apps we found were Scaniverse and \tdsapp.
# , both
# available on the Apple App Store.  
#
# Both mark regions of interest on
# the screen where the features seen by the RGB camera are not yet
# detected by the LiDAR. These markings slowly disappear as the region
# is exposed from different points of view and distances.  
#
For \tdsapp,
the best settings were high resolution (5mm), maximum depth
5m, confidence level high or medium (depending on the plants),
and masking set to off.   For Scaniverse, the only settable parameter was the
maximum depth, which we set to 5m, and we have no information on the app's
default point cloud resolution.
#
We scanned isolated maize plants from the ground to the tassel, walking
around the plant to capture data at different heights and distances from
the culm. For soybeans and cocklebur, we scanned from the top of the canopy
downwards, circling around the plant.  We imaged these at distances ranging
from 5cm -- 1m, while we imaged maize at about 3m from the culm.  All scans
used an oblique angle and included the soil for ample ground reflections.


Scans of cocklebur and soybeans show their canopies reflect the LiDAR
well, but the lower leaves and the stems were invisible to LiDAR.
When set to high confidence, \tdsapp returned fewer features than at
medium confidence; conversely, its 3D reconstruction was better at the
higher confidence.  Scaniverse's default higher confidence level detects
fewer features, producing a poorer 3D reconstruction than \tdsapp.
#
# Neither app returned good images when the confidence level was set to
# high.  3d Scanner App at medium confidence found more features, but the
# 3D reconstruction was poor.  
#
In contrast, maize was scanned better with Scaniverse than with \tdsapp.
Plant features more proximal to the culm, such as the ear and the leaf
sheath, were reconstructed better than more distal portions of the leaves.
#
Senesced maize --- culms, ears, and leaves --- reflected better
compared to younger, greener leaves and tassels.
#
These results are consistent with prior observations: thicker, denser plant
structures reflect better than thinner, less dense ones.  As maize
senesces, it dries and shrinks, likely increasing the density of the plant
material.  For the bushier plants, we could partially compensate for the
canopy occluding the laser by scanning alongside the plant at close range,
but this strategy failed with maize.  Reconstruction may be better with
better instrumentation, but it seems likely that better models of plant
reflectances, gap probabilities, and leaves will be needed for each crop
species of interest.


\vspace{-3.5mm}

#+begin_export latex :noexport:

% fixed vertical alignment with graphbox per
% https://tex.stackexchange.com/questions/19080/how-to-vertically-center-text-with-an-image-in-the-same-row-of-a-table
%
% Kazic, 2.10.2021



\begin{figure}[!b]
\centering
%
\begin{tabular}{cccc}
%
% this is the transpose: plants as rows and outputs as columns
%
% rgb
%

& & \multicolumn{2}{c}{raw point clouds} \\
& \textsc{RGB} & \tdsapp & Scaniverse \\
%
\emph{soybean} &
\scalebox{0.08}{\includegraphics[align=c]{./images/meshlab/soybean3_rgb.png}} &
\scalebox{0.10}{\includegraphics[align=c]{./images/meshlab/soybean3_3dscanner_pc.png}} &
\scalebox{0.10}{\includegraphics[align=c]{./images/meshlab/soybean3_scaniverse_pc.png}} \\
%
\emph{cocklebur} &
\scalebox{0.175}{\includegraphics[align=c]{./images/meshlab/cocklebur2_rgb.png}} &
\scalebox{0.11}{\includegraphics[align=c]{./images/meshlab/cocklebur2_3dscanner_pc.png}} &
\scalebox{0.115}{\includegraphics[align=c]{./images/meshlab/cocklebur2_scaniverse_pc.png}} \\
%
\emph{maize} &
\scalebox{0.14}{\includegraphics[align=c]{./images/meshlab/corn2_rgb.png}} &
\scalebox{0.125}{\includegraphics[align=c]{./images/meshlab/corn2_3dScanner_pc.png}} &
\scalebox{0.16}{\includegraphics[align=c]{./images/meshlab/corn2_scaniverse_pc.png}} \\
%
\end{tabular}
% \caption{3D reconstructions by 3d Scanner App (top) and Scaniverse (bottom).  Left panels, soybean; center panels, cocklebur; right panels, maize.}
\caption{RGB images from the video and raw point clouds from the two apps.  Orange marking flags are visible in the cocklebur and the RGB and \tdsapp maize panels.}
\label{app-bakeoff}
\end{figure}

#+end_export











#+begin_export latex :noexport:

% this is the original, untransposed

% fixed vertical alignment with graphbox per
% https://tex.stackexchange.com/questions/19080/how-to-vertically-center-text-with-an-image-in-the-same-row-of-a-table
%
% Kazic, 2.10.2021



\begin{figure}[!b]
\centering
%
\begin{tabular}{cccc}
%
% a transpose has plants as rows and outputs as columns
%
% rgb
%
\textsc{RGB} &
\scalebox{0.14}{\includegraphics[align=c]{./images/meshlab/soybean3_rgb.png}} &
\scalebox{0.30}{\includegraphics[align=c]{./images/meshlab/cocklebur2_rgb.png}} &
\scalebox{0.20}{\includegraphics[align=c]{./images/meshlab/corn2_rgb.png}} \\
%
% 3d scanner app -- scaling correct
%
3dScanner App &
\scalebox{0.11}{\includegraphics[align=c]{./images/meshlab/soybean3_3dscanner.png}} &
\scalebox{0.12}{\includegraphics[align=c]{./images/meshlab/cocklebur2_3dscanner.png}} &
\scalebox{0.19}{\includegraphics[align=c]{./images/meshlab/corn2_3dScanner.png}} \\
%
% scaniverse -- scaling correct
%
Scaniverse &
\scalebox{0.11}{\includegraphics[align=c]{./images/meshlab/soybean3_scaniverse.png}} &
\scalebox{0.12}{\includegraphics[align=c]{./images/meshlab/cocklebur2_scanniverse.png}} &
\scalebox{0.23}{\includegraphics[align=c]{./images/meshlab/corn2_scaniverse.png}} \\
& \emph{soybean} & \emph{cocklebur} & \emph{maize} \\
\end{tabular}
% \caption{3D reconstructions by 3d Scanner App (top) and Scaniverse (bottom).  Left panels, soybean; center panels, cocklebur; right panels, maize.}
\caption{RGB images and 3D reconstructions.}
\label{app-bakeoff}
\end{figure}

#+end_export





# #
# #
# #+caption: 3D reconstructions by 3d Scanner App (top) and Scaniverse (bottom).  Left panels, soybean; center panels, cocklebur; right panels, maize.
# #+name: app-bakeoff
# #+attr_latex: :placement [!b]
# [[./images/soybean_medium2_3d.png]] [[./images/coclebirdflag1_3d.png]] [[./images/cornflag_3s.png]] 
# [[./images/soybean_scani.png]] [[./images/cocklebur1_scani.png]] [[./images/corn_orange_flag1_scani.png]]
# #
# #



# maize scani
# cock 3d
# soy 3d






# \vspace{1cm}




#+BEGIN_EXPORT latex

\renewcommand{\refname}{\bfseries\selectfont\normalsize References} 

\normalsize

\bibliographystyle{chicago-ff}

\bibliography{\bp{journals},%
              \bp{keys},%
              \bp{miscellaneous},%
%              \bp{clean-egbib},%
              \bp{all}}



#+END_EXPORT



* abstract :dewi: :noexport: 

   + Agricultural 3D reconstruction is often achieved by using structure
     from motion of high resolutions RGB imagery from UAV or hand held
     cameras \cite{guo2021,che2020}. 

   + Light Detectior and Ranging (LiDAR) is a depth sensor-based device
     that uses a pulsed laser to measure distances to the surfaces of
     objects. It is less sensitive to lighting conditions than RGB imagery,
     making it a good candidate for agriculture and outdoor applications.

   + LiDAR sensors are often mounted on  aerial vehicles to get point clouds
     that reconstruct 3D canopies of the area of interest
     \cite{schneider2014,yin2020}.  Successful reconstruction depends on
     having an adequate model for the reflectances and scattering of the
     vegetation, the relevant scanner and image acquisition parameters, and
     good simulations of expected waveforms, point clouds, and intensities.

   + Morphological phenotypes, such as plant height, leaf pathologies and
     dimensions, and the status of reproductive organs are important to
     assess as the plant develops for research in genetics and crop
     improvement and for managing production fields \cite{kelly2016a}. In
     principle, LiDAR data could provide detailed morphological information
     for a variety of crops, and the dimensions obtained from LiDAR-based
     morphological models could be used to correct morphological models
     built from other data. However, reflections from beneath the canopy,
     for example for stems and lower leaves, are currently difficult to
     resolve underneath the complex oclusions.
     

   + Relatively high resolution LiDAR scanners are scarce and
     expensive. The recent availability of consumer-grade LiDAR devices on
     Apple iPhones and iPads might provide a way to collect close-range,
     higher resolution data close to the plant, rather than imaging through
     the canopy.  Gollob /et alia/ compared data on the diameter of trees
     at breast height (dbh, a standard forestry measure) collected with a
     2020 iPad Pro to standard methods and found remarkably good agreement
     \cite{gollob2021}.  

   + We explored the use of a 2021 Apple iPad Pro, different methods of
     data collection, and different free apps on maize, soybeans, and
     common field weeds late in the 2021 field season.  These plants have
     very different architectures: maize is tall, with relatively few,
     large, fairly thick, and quite narrow leaves; soybeans are short and
     bushy, with many small, thin, roughly circular leaves.  Cocklebur, a
     common field weed, is intermediate: taller than most soybean
     varieties, with much larger circular and thicker leaves, and a more
     open, branching stem.


   + The two best apps we found were Scaniverse and 3d Scanner App, both
     available on the Apple App Store.  Both mark regions of interest on
     the screen where the features seen in the RGB camera are not yet
     detected by the LiDAR. These markings slowly dissapear as the region
     is exposed from different points of view and distances.  For 3d Scanner
     App, the best settings were high resolution (5mm), maximum depth
     5m, confidence level high or medium (depending on the plants),
     and masking off.   For Scaniverse, the only settable parameter was the
     maximum depth, which we set to 5m.

   + We scanned isolated maize plants from the ground to the tassel,
     walking around the plant to capture data at different heights and
     distances from the culm. For soybeans and cocklebur, we followed a
     similar procedure, but were able to get *much closer* to the plant
     than maize *dewi????*.  All scans used an oblique angle and included
     the soil for ample ground reflections.


   + Scans of cocklebur and soybeans show their canopies reflect the LiDAR
     well, but the lower leaves and the stems were invisible to LiDAR.
     Neither app returned good images when the confidence level was set to
     high.  3d Scanner App at medium confidence found more features, but the
     3D reconstruction was poor.  Compared to these species, maize did not
     scan well at all with either app.  Most of the plant was invisible to
     LiDAR under the best of conditions.  Senesced maize --- culms, ears,
     and leaves --- reflected well compared to younger, greener leaves and
     tassels.

   + These results are consistent with prior observations: thicker, denser
     plant structures reflect better than thinner, less dense ones.  As
     maize senesces, it dries and shrinks, likely increasing the density of
     the plant material.  For the bushier plants, we could partially
     compensate for the canopy occluding the laser by scanning alongside
     the plant at close range, but this strategy failed with maize.

   + Reconstruction may be better with better instrumentation, but it seems
     likely that better models of plant reflectances, gap probabilities,
     and leaves will be needed for each crop species of interest.


   # + Scaniverse and 3D Scanner App with high confidence failed to scan
   #   thinner plans. Medium confidence level on 3D scanner app can catch
   #   better features but the 3D results are poor. 

# The LiDAR
#      reflects stem, ear, and brown leaves well, but it has difficulties
#      scanning greener leaves and tasel. 


#   + In Scaniverse, we use $5$m max depth.

# we would like to have another type of data for 3D reconstruction of
#      plant morphologies in the field to serve as ground truth --- tried
#      LiDAR based on previous work, especially in forests
#      \cite{gollob2021}. 


#    + Not only canopies, but we want the whole plant morphologies with clear
#      organs visible.  
#    + LiDAR gives dimensions directly, so the data could help scale
#      reconstructions from other data
#    + Good scanners are scarce and expensive.
#    + Apple introducing devices with LiDAR sensor embeded in 2020. This
#      enable the access of LiDAR based 3D point cloud in consumer grade
#      devices. For this study, we are using Apple iPad Pro 2021 with free
#      LiDAR applications available on Apple Store 

 
#    + We explored different free apps and different procedure for data
#     collections. 


#   + 2 best applications are Scaniverse and 3D Scanner App


   # + Data collection: Both applications provides grids on the screen where
   #   the features are not detected yet in the area of interest. These grids
   #   will slowly dissapear when we exposed the area from different point of
   #   views and distance.
#   + we imaged plants with different fundamental architectures
#   + plants with thick organs reflect better than skinny plants -- e.g.,
     soybeans, cocklebur /vs./ maize, consistent with forest canopy imagery 
  



   # + On soybean, it scans the surface of the canopy well, but failed on
   #   leaves lower layers and stem even though we brought the LiDAR really
   #   close to them. 
   # + Clearly, the light pulse went through the surface of thin vegetation. 
   # + But, \cite{gollob2021} has a really good
   #   result since trees at breast level has very thick and solid stem that
   #   enable the pulse to bounce back. 
  


* figures  :dewi:                                                         :noexport:

   + figures: meshlab for point clouds
   + 3dscanner for rgb and point clouds
   + weeds, soybeans, and maize
   

* references :toni:  :noexport: 

read first and figure out relevance and what to say about them before horsing into jabref

   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/schneider2014.pdf ][schneider2014]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/yin2020.pdf ][yin2020]]
   + [[file:../../../../../../Dropbox/graphos_reprints/image_processing/3d_reconstructn/gollob2021.pdf][gollob2021]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/drone/guo2021.pdf][guo2021]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/sofonia2019.pdf][sofonia2019]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/lin2021.pdf][lin2021]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/christiansen2017.pdf][christiansen2017]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/shendryk2020.pdf][shendryk2020]] 
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/zhou2020b.pdf][zhou2020b]]
   + [[file:../../../../../../Dropbox/graphos_reprints/phenotyping/lidar_for_vegetatn/hu2020a.pdf][hu2020a]]
   + [[https://www.sciencedirect.com/science/article/abs/pii/S0924271620303130][S. Jin, X. Sun, F. Wu et al.]], “Lidar sheds new light on plant
phenomics for plant breeding and management: recent advances and future
prospects,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 171,
pp. 202–223, 2021.  pretty lightweight
   + [[https://vision.eng.au.dk/future-cropping/uav_lidar/][Aarhus group]]



* abstract mechanics                                               :noexport:

 this link

https://cmt3.research.microsoft.com/MLCAS2021/Track/1/Submission/Create


   + up to 2 p extended abstract, including figures, tables and
     references. [[file:./MLCAS2021-template.docx][Extended abstract template in word]], [[file:./MLCAS2021-template.pdf][pdf]].
   + can include figures
   + due oct 1
   + they take pdf!!!
   + probably easiest is to write in org and then export to latex, and
     configure a template preamble
   + [[https://mlcas2021.github.io/][website]]
   + [[../../done/posters/maize_mtg19/poster.org][poster with tikzposter]]
   + [[https://tex.stackexchange.com/questions/516029/media9-is-becoming-obsolete-dec-2020-any-alternatives-for-embedding-video-audio/][video post-flash demise, scroll down]];  [[https://tex.stackexchange.com/questions/577986/alternatives-for-media9-after-dec-2020-in-overleaf][post media9]];  [[https://www.youtube.com/watch?v=y_N1A1yX0xg][beamer video in
     2021]];  [[https://ctan.math.washington.edu/tex-archive/macros/latex/contrib/media9/doc/media9.pdf][media9 2021]]
   + Rumana also used ba4poster at one point
   + [[http://tug.ctan.org/macros/latex/contrib/animate/animate.pdf][animate latex pkg]]
